{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Xception Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Cy5aZh3xbnQB7a5ZpPDXU3GwGWgAIXAc",
      "authorship_tag": "ABX9TyPJAZ5mtOtY+Q2hbRSKxwPj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PinkyChaudhary00/Semotions/blob/main/Xception_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjQ4-McoFVkb"
      },
      "source": [
        "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import SeparableConv2D\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi9-O5rFFWdq"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRqF8gTWq2Ff"
      },
      "source": [
        "dataset_path = '/content/drive/MyDrive/fer2013/fer2013.csv'\n",
        "\n",
        "def load_fer2013(dataset_path):\n",
        "        data = pd.read_csv(dataset_path)\n",
        "        pixels = data['pixels'].tolist()\n",
        "        width, height = 48, 48\n",
        "        faces = []\n",
        "        for pixel_sequence in pixels:\n",
        "            face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "            face = np.asarray(face).reshape(width, height)\n",
        "            face = cv2.resize(face.astype('float32'),(100,100))\n",
        "            face = cv2.cvtColor(face,cv2.COLOR_GRAY2RGB)\n",
        "            faces.append(face)\n",
        "        faces = np.asarray(faces)\n",
        "        emotions = pd.get_dummies(data['emotion']).to_numpy()\n",
        "        return faces, emotions\n",
        "        \n",
        "faces,emotions = load_fer2013(dataset_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0flycfOwmVB"
      },
      "source": [
        "def split_data(x, y):\n",
        "    num_samples = len(x)\n",
        "    num_train_samples = int(0.8*num_samples)\n",
        "    train_x = x[:num_train_samples]\n",
        "    train_y = y[:num_train_samples]\n",
        "    val_x = x[num_train_samples:]\n",
        "    val_y = y[num_train_samples:]\n",
        "    train_data = (train_x, train_y)\n",
        "    val_data = (val_x, val_y)\n",
        "    return train_data, val_data\n",
        "train_data, val_data = split_data(faces, emotions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK3ybLqExCaw",
        "outputId": "14553572-24b8-4204-fe6e-b4e74bdfe412"
      },
      "source": [
        "# Load data sets\n",
        "(X_train, y_train), (X_test, y_test) = train_data,val_data\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28709, 100, 100, 3)\n",
            "(7178, 100, 100, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkt2EeCEwMmX"
      },
      "source": [
        "# Define Image Data Generator for training set\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=5,\n",
        "                                   shear_range=0.1,\n",
        "                                   zoom_range=0.2, \n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "# Define Image Generator for test set\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "m3sKxUmgw8lB",
        "outputId": "1519ae5a-9ab7-4707-c7d4-24deab4ab614"
      },
      "source": [
        "for X_batch, y_batch in train_datagen.flow(X_train, y_train, batch_size=9, seed=42):\n",
        "    # create a grid of 3x3 images\n",
        "    for i in range(0, 9):\n",
        "        plt.subplot(330 + 1 + i)\n",
        "        plt.imshow(X_batch[i])\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "    break\t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEmHkPD8GoKr"
      },
      "source": [
        "def tiny_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n",
        "    regularization = l2(l2_regularization)\n",
        "\n",
        "    # base\n",
        "    img_input = Input(input_shape)\n",
        "    x = Conv2D(5, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
        "               use_bias=False)(img_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(5, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
        "               use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # module 1\n",
        "    residual = Conv2D(8, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(8, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(8, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 2\n",
        "    residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 3\n",
        "    residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 4\n",
        "    residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    x = Conv2D(num_classes, (3, 3),\n",
        "               # kernel_regularizer=regularization,\n",
        "               padding='same')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    output = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(img_input, output)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwvvB7QHG0MU",
        "outputId": "8f24c816-7833-46b2-cd38-cacea5a1b084"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_shape = (100, 100, 3)\n",
        "    num_classes = 7\n",
        "    model = tiny_XCEPTION(input_shape, num_classes)\n",
        "    model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 98, 98, 5)    135         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 98, 98, 5)    20          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 98, 98, 5)    0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 96, 96, 5)    225         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 96, 96, 5)    20          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 96, 96, 5)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d (SeparableConv (None, 96, 96, 8)    85          activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 96, 96, 8)    32          separable_conv2d[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 96, 96, 8)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 96, 96, 8)    136         activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 96, 96, 8)    32          separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 48, 48, 8)    40          activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 48, 48, 8)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 48, 48, 8)    32          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 48, 48, 8)    0           max_pooling2d[0][0]              \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 48, 48, 16)   200         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 48, 48, 16)   64          separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 48, 48, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 48, 48, 16)   400         activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 48, 48, 16)   64          separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 24, 24, 16)   128         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 24, 24, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 24, 24, 16)   0           max_pooling2d_1[0][0]            \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 24, 24, 32)   656         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 24, 24, 32)   128         separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 24, 24, 32)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 24, 24, 32)   1312        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 24, 24, 32)   128         separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 12, 12, 32)   512         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 32)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 12, 12, 32)   128         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 12, 12, 32)   0           max_pooling2d_2[0][0]            \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 12, 12, 64)   2336        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 12, 12, 64)   256         separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 12, 12, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 12, 12, 64)   4672        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 12, 12, 64)   256         separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 6, 6, 64)     256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]            \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 6, 6, 7)      4039        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 7)            0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Activation)        (None, 7)            0           global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 18,404\n",
            "Trainable params: 17,664\n",
            "Non-trainable params: 740\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc1SuNCRIkFC"
      },
      "source": [
        "verbose = 1\n",
        "num_classes = 7\n",
        "patience = 50\n",
        "batch_size = 32\n",
        "num_epochs = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaU0cIlQIHXi"
      },
      "source": [
        "log_file_path = 'emotion_training.log'\n",
        "csv_logger = CSVLogger(log_file_path, append=False)\n",
        "early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
        "model_names = 'model' + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
        "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwUp1ouQIrd5"
      },
      "source": [
        "model = tiny_XCEPTION((100,100,3), num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ucJIZeoYIZ23",
        "outputId": "f1c7ed8e-c69d-4cf5-963e-e580eadc177a"
      },
      "source": [
        "history = model.fit(train_datagen.flow(X_train, y_train, batch_size), \n",
        "                    validation_data=test_datagen.flow(X_test, y_test, batch_size=32),\n",
        "                    callbacks=callbacks,epochs= num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "898/898 [==============================] - 85s 93ms/step - loss: 1.8660 - accuracy: 0.2733 - val_loss: 1.7358 - val_accuracy: 0.3377\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.73585, saving model to model.01-0.34.hdf5\n",
            "Epoch 2/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.5957 - accuracy: 0.3998 - val_loss: 1.5705 - val_accuracy: 0.3994\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.73585 to 1.57052, saving model to model.02-0.40.hdf5\n",
            "Epoch 3/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.4846 - accuracy: 0.4425 - val_loss: 1.5899 - val_accuracy: 0.3803\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.57052\n",
            "Epoch 4/10000\n",
            "898/898 [==============================] - 84s 93ms/step - loss: 1.4146 - accuracy: 0.4692 - val_loss: 1.5654 - val_accuracy: 0.4299\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.57052 to 1.56544, saving model to model.04-0.43.hdf5\n",
            "Epoch 5/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.3571 - accuracy: 0.4940 - val_loss: 1.6976 - val_accuracy: 0.4271\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.56544\n",
            "Epoch 6/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.3170 - accuracy: 0.5051 - val_loss: 1.6936 - val_accuracy: 0.3997\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.56544\n",
            "Epoch 7/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.2934 - accuracy: 0.5165 - val_loss: 1.3046 - val_accuracy: 0.5164\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.56544 to 1.30456, saving model to model.07-0.52.hdf5\n",
            "Epoch 8/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 1.2739 - accuracy: 0.5154 - val_loss: 1.6268 - val_accuracy: 0.4096\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.30456\n",
            "Epoch 9/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 1.2504 - accuracy: 0.5318 - val_loss: 1.4740 - val_accuracy: 0.4395\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.30456\n",
            "Epoch 10/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.2354 - accuracy: 0.5369 - val_loss: 1.4276 - val_accuracy: 0.4877\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.30456\n",
            "Epoch 11/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.2218 - accuracy: 0.5443 - val_loss: 1.3866 - val_accuracy: 0.4985\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.30456\n",
            "Epoch 12/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 1.1861 - accuracy: 0.5580 - val_loss: 1.3264 - val_accuracy: 0.5065\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.30456\n",
            "Epoch 13/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.2008 - accuracy: 0.5496 - val_loss: 1.1980 - val_accuracy: 0.5482\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.30456 to 1.19802, saving model to model.13-0.55.hdf5\n",
            "Epoch 14/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1828 - accuracy: 0.5526 - val_loss: 1.2040 - val_accuracy: 0.5468\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.19802\n",
            "Epoch 15/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1771 - accuracy: 0.5602 - val_loss: 1.4857 - val_accuracy: 0.4547\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.19802\n",
            "Epoch 16/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1645 - accuracy: 0.5634 - val_loss: 1.2225 - val_accuracy: 0.5426\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.19802\n",
            "Epoch 17/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1660 - accuracy: 0.5629 - val_loss: 1.3082 - val_accuracy: 0.5380\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.19802\n",
            "Epoch 18/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.1558 - accuracy: 0.5674 - val_loss: 1.2574 - val_accuracy: 0.5400\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.19802\n",
            "Epoch 19/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1419 - accuracy: 0.5723 - val_loss: 1.3655 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.19802\n",
            "Epoch 20/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1282 - accuracy: 0.5781 - val_loss: 1.6361 - val_accuracy: 0.4291\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.19802\n",
            "Epoch 21/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1363 - accuracy: 0.5753 - val_loss: 1.1775 - val_accuracy: 0.5588\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.19802 to 1.17748, saving model to model.21-0.56.hdf5\n",
            "Epoch 22/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1257 - accuracy: 0.5798 - val_loss: 1.2195 - val_accuracy: 0.5424\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.17748\n",
            "Epoch 23/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1202 - accuracy: 0.5772 - val_loss: 1.2289 - val_accuracy: 0.5414\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.17748\n",
            "Epoch 24/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1142 - accuracy: 0.5829 - val_loss: 1.1793 - val_accuracy: 0.5619\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.17748\n",
            "Epoch 25/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.1081 - accuracy: 0.5859 - val_loss: 1.3325 - val_accuracy: 0.5146\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.17748\n",
            "Epoch 26/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.1085 - accuracy: 0.5837 - val_loss: 1.2649 - val_accuracy: 0.5159\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.17748\n",
            "Epoch 27/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1019 - accuracy: 0.5877 - val_loss: 1.2712 - val_accuracy: 0.5421\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.17748\n",
            "Epoch 28/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0858 - accuracy: 0.5909 - val_loss: 1.1645 - val_accuracy: 0.5723\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.17748 to 1.16454, saving model to model.28-0.57.hdf5\n",
            "Epoch 29/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.1028 - accuracy: 0.5885 - val_loss: 1.1801 - val_accuracy: 0.5599\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.16454\n",
            "Epoch 30/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 1.0944 - accuracy: 0.5889 - val_loss: 1.1671 - val_accuracy: 0.5743\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.16454\n",
            "Epoch 31/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0778 - accuracy: 0.5994 - val_loss: 1.1891 - val_accuracy: 0.5619\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.16454\n",
            "Epoch 32/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0902 - accuracy: 0.5924 - val_loss: 1.1453 - val_accuracy: 0.5736\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.16454 to 1.14532, saving model to model.32-0.57.hdf5\n",
            "Epoch 33/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0869 - accuracy: 0.5928 - val_loss: 1.2074 - val_accuracy: 0.5405\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.14532\n",
            "Epoch 34/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0695 - accuracy: 0.6030 - val_loss: 1.1780 - val_accuracy: 0.5596\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.14532\n",
            "Epoch 35/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0876 - accuracy: 0.5932 - val_loss: 1.2026 - val_accuracy: 0.5637\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.14532\n",
            "Epoch 36/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.0733 - accuracy: 0.5986 - val_loss: 1.1429 - val_accuracy: 0.5723\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.14532 to 1.14287, saving model to model.36-0.57.hdf5\n",
            "Epoch 37/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 1.0685 - accuracy: 0.5993 - val_loss: 1.3254 - val_accuracy: 0.5128\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.14287\n",
            "Epoch 38/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0754 - accuracy: 0.5984 - val_loss: 1.1290 - val_accuracy: 0.5754\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.14287 to 1.12901, saving model to model.38-0.58.hdf5\n",
            "Epoch 39/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0571 - accuracy: 0.6074 - val_loss: 1.2524 - val_accuracy: 0.5471\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.12901\n",
            "Epoch 40/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0693 - accuracy: 0.6062 - val_loss: 1.3320 - val_accuracy: 0.5088\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.12901\n",
            "Epoch 41/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0587 - accuracy: 0.6066 - val_loss: 1.0989 - val_accuracy: 0.5913\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.12901 to 1.09891, saving model to model.41-0.59.hdf5\n",
            "Epoch 42/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 1.0535 - accuracy: 0.6051 - val_loss: 1.1597 - val_accuracy: 0.5766\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.09891\n",
            "Epoch 43/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0611 - accuracy: 0.6073 - val_loss: 1.2098 - val_accuracy: 0.5532\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.09891\n",
            "Epoch 44/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0587 - accuracy: 0.6074 - val_loss: 1.1319 - val_accuracy: 0.5794\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.09891\n",
            "Epoch 45/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 1.0440 - accuracy: 0.6097 - val_loss: 1.1968 - val_accuracy: 0.5641\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.09891\n",
            "Epoch 46/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.0486 - accuracy: 0.6060 - val_loss: 1.3103 - val_accuracy: 0.5233\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.09891\n",
            "Epoch 47/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.0529 - accuracy: 0.6089 - val_loss: 1.2543 - val_accuracy: 0.5475\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.09891\n",
            "Epoch 48/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0496 - accuracy: 0.6096 - val_loss: 1.1624 - val_accuracy: 0.5762\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.09891\n",
            "Epoch 49/10000\n",
            "898/898 [==============================] - 84s 94ms/step - loss: 1.0469 - accuracy: 0.6083 - val_loss: 1.1020 - val_accuracy: 0.5860\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.09891\n",
            "Epoch 50/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0327 - accuracy: 0.6157 - val_loss: 1.1766 - val_accuracy: 0.5607\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.09891\n",
            "Epoch 51/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0343 - accuracy: 0.6052 - val_loss: 1.1341 - val_accuracy: 0.5818\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.09891\n",
            "Epoch 52/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 1.0392 - accuracy: 0.6111 - val_loss: 1.1160 - val_accuracy: 0.5818\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.09891\n",
            "Epoch 53/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0357 - accuracy: 0.6103 - val_loss: 1.2090 - val_accuracy: 0.5599\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.09891\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 54/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 1.0123 - accuracy: 0.6219 - val_loss: 1.0521 - val_accuracy: 0.6069\n",
            "\n",
            "Epoch 00054: val_loss improved from 1.09891 to 1.05207, saving model to model.54-0.61.hdf5\n",
            "Epoch 55/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 0.9858 - accuracy: 0.6334 - val_loss: 1.0523 - val_accuracy: 0.6057\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.05207\n",
            "Epoch 56/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9924 - accuracy: 0.6289 - val_loss: 1.0430 - val_accuracy: 0.6138\n",
            "\n",
            "Epoch 00056: val_loss improved from 1.05207 to 1.04302, saving model to model.56-0.61.hdf5\n",
            "Epoch 57/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9834 - accuracy: 0.6363 - val_loss: 1.0478 - val_accuracy: 0.6091\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.04302\n",
            "Epoch 58/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9784 - accuracy: 0.6368 - val_loss: 1.0442 - val_accuracy: 0.6149\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.04302\n",
            "Epoch 59/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9738 - accuracy: 0.6401 - val_loss: 1.0420 - val_accuracy: 0.6113\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.04302 to 1.04195, saving model to model.59-0.61.hdf5\n",
            "Epoch 60/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 0.9937 - accuracy: 0.6333 - val_loss: 1.0431 - val_accuracy: 0.6130\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.04195\n",
            "Epoch 61/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 0.9821 - accuracy: 0.6310 - val_loss: 1.0462 - val_accuracy: 0.6134\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.04195\n",
            "Epoch 62/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 0.9727 - accuracy: 0.6406 - val_loss: 1.0442 - val_accuracy: 0.6095\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.04195\n",
            "Epoch 63/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9788 - accuracy: 0.6396 - val_loss: 1.0469 - val_accuracy: 0.6113\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.04195\n",
            "Epoch 64/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9724 - accuracy: 0.6388 - val_loss: 1.0635 - val_accuracy: 0.6126\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.04195\n",
            "Epoch 65/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9645 - accuracy: 0.6419 - val_loss: 1.0424 - val_accuracy: 0.6159\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.04195\n",
            "Epoch 66/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9841 - accuracy: 0.6352 - val_loss: 1.0442 - val_accuracy: 0.6180\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.04195\n",
            "Epoch 67/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9632 - accuracy: 0.6393 - val_loss: 1.0413 - val_accuracy: 0.6176\n",
            "\n",
            "Epoch 00067: val_loss improved from 1.04195 to 1.04132, saving model to model.67-0.62.hdf5\n",
            "Epoch 68/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9630 - accuracy: 0.6439 - val_loss: 1.0449 - val_accuracy: 0.6174\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.04132\n",
            "Epoch 69/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9801 - accuracy: 0.6354 - val_loss: 1.0457 - val_accuracy: 0.6177\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.04132\n",
            "Epoch 70/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9715 - accuracy: 0.6392 - val_loss: 1.0480 - val_accuracy: 0.6094\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.04132\n",
            "Epoch 71/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9627 - accuracy: 0.6430 - val_loss: 1.0479 - val_accuracy: 0.6117\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.04132\n",
            "Epoch 72/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9606 - accuracy: 0.6472 - val_loss: 1.0484 - val_accuracy: 0.6130\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.04132\n",
            "Epoch 73/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9665 - accuracy: 0.6398 - val_loss: 1.0775 - val_accuracy: 0.6018\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.04132\n",
            "Epoch 74/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9655 - accuracy: 0.6420 - val_loss: 1.0428 - val_accuracy: 0.6138\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.04132\n",
            "Epoch 75/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9623 - accuracy: 0.6393 - val_loss: 1.0505 - val_accuracy: 0.6067\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.04132\n",
            "Epoch 76/10000\n",
            "898/898 [==============================] - 85s 94ms/step - loss: 0.9640 - accuracy: 0.6441 - val_loss: 1.0389 - val_accuracy: 0.6147\n",
            "\n",
            "Epoch 00076: val_loss improved from 1.04132 to 1.03895, saving model to model.76-0.61.hdf5\n",
            "Epoch 77/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9585 - accuracy: 0.6461 - val_loss: 1.0396 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.03895\n",
            "Epoch 78/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9559 - accuracy: 0.6482 - val_loss: 1.0360 - val_accuracy: 0.6160\n",
            "\n",
            "Epoch 00078: val_loss improved from 1.03895 to 1.03602, saving model to model.78-0.62.hdf5\n",
            "Epoch 79/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9748 - accuracy: 0.6361 - val_loss: 1.0380 - val_accuracy: 0.6152\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.03602\n",
            "Epoch 80/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9689 - accuracy: 0.6389 - val_loss: 1.0472 - val_accuracy: 0.6163\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.03602\n",
            "Epoch 81/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9652 - accuracy: 0.6413 - val_loss: 1.0489 - val_accuracy: 0.6142\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.03602\n",
            "Epoch 82/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9748 - accuracy: 0.6383 - val_loss: 1.0457 - val_accuracy: 0.6126\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.03602\n",
            "Epoch 83/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9489 - accuracy: 0.6416 - val_loss: 1.0432 - val_accuracy: 0.6163\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.03602\n",
            "Epoch 84/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9543 - accuracy: 0.6483 - val_loss: 1.0491 - val_accuracy: 0.6166\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.03602\n",
            "Epoch 85/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9582 - accuracy: 0.6449 - val_loss: 1.0506 - val_accuracy: 0.6112\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.03602\n",
            "Epoch 86/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9685 - accuracy: 0.6428 - val_loss: 1.0414 - val_accuracy: 0.6176\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.03602\n",
            "Epoch 87/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9730 - accuracy: 0.6340 - val_loss: 1.0447 - val_accuracy: 0.6176\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.03602\n",
            "Epoch 88/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9626 - accuracy: 0.6394 - val_loss: 1.0417 - val_accuracy: 0.6138\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.03602\n",
            "Epoch 89/10000\n",
            "898/898 [==============================] - 86s 95ms/step - loss: 0.9730 - accuracy: 0.6398 - val_loss: 1.0362 - val_accuracy: 0.6148\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.03602\n",
            "Epoch 90/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9644 - accuracy: 0.6396 - val_loss: 1.0334 - val_accuracy: 0.6184\n",
            "\n",
            "Epoch 00090: val_loss improved from 1.03602 to 1.03336, saving model to model.90-0.62.hdf5\n",
            "Epoch 91/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9477 - accuracy: 0.6461 - val_loss: 1.0332 - val_accuracy: 0.6160\n",
            "\n",
            "Epoch 00091: val_loss improved from 1.03336 to 1.03319, saving model to model.91-0.62.hdf5\n",
            "Epoch 92/10000\n",
            "898/898 [==============================] - 86s 96ms/step - loss: 0.9672 - accuracy: 0.6407 - val_loss: 1.0444 - val_accuracy: 0.6166\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.03319\n",
            "Epoch 93/10000\n",
            "898/898 [==============================] - 86s 95ms/step - loss: 0.9621 - accuracy: 0.6392 - val_loss: 1.0378 - val_accuracy: 0.6159\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.03319\n",
            "Epoch 94/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9506 - accuracy: 0.6479 - val_loss: 1.0406 - val_accuracy: 0.6166\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.03319\n",
            "Epoch 95/10000\n",
            "898/898 [==============================] - 86s 95ms/step - loss: 0.9636 - accuracy: 0.6415 - val_loss: 1.0441 - val_accuracy: 0.6159\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.03319\n",
            "Epoch 96/10000\n",
            "898/898 [==============================] - 86s 96ms/step - loss: 0.9604 - accuracy: 0.6353 - val_loss: 1.0409 - val_accuracy: 0.6141\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.03319\n",
            "Epoch 97/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9501 - accuracy: 0.6433 - val_loss: 1.0384 - val_accuracy: 0.6152\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.03319\n",
            "Epoch 98/10000\n",
            "898/898 [==============================] - 85s 95ms/step - loss: 0.9532 - accuracy: 0.6423 - val_loss: 1.0464 - val_accuracy: 0.6134\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.03319\n",
            "Epoch 99/10000\n",
            "898/898 [==============================] - 86s 95ms/step - loss: 0.9640 - accuracy: 0.6417 - val_loss: 1.0378 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.03319\n",
            "Epoch 100/10000\n",
            "898/898 [==============================] - 86s 95ms/step - loss: 0.9549 - accuracy: 0.6468 - val_loss: 1.0527 - val_accuracy: 0.6113\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.03319\n",
            "Epoch 101/10000\n",
            "898/898 [==============================] - 86s 95ms/step - loss: 0.9533 - accuracy: 0.6461 - val_loss: 1.0379 - val_accuracy: 0.6151\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.03319\n",
            "Epoch 102/10000\n",
            "898/898 [==============================] - 86s 96ms/step - loss: 0.9575 - accuracy: 0.6459 - val_loss: 1.0358 - val_accuracy: 0.6142\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.03319\n",
            "Epoch 103/10000\n",
            "898/898 [==============================] - 86s 96ms/step - loss: 0.9514 - accuracy: 0.6456 - val_loss: 1.0507 - val_accuracy: 0.6133\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.03319\n",
            "\n",
            "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 104/10000\n",
            "898/898 [==============================] - 86s 96ms/step - loss: 0.9598 - accuracy: 0.6458 - val_loss: 1.0361 - val_accuracy: 0.6184\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.03319\n",
            "Epoch 105/10000\n",
            "898/898 [==============================] - 86s 96ms/step - loss: 0.9492 - accuracy: 0.6501 - val_loss: 1.0339 - val_accuracy: 0.6191\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.03319\n",
            "Epoch 106/10000\n",
            "327/898 [=========>....................] - ETA: 53s - loss: 0.9479 - accuracy: 0.6494"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-59b3ed5f6082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(train_datagen.flow(X_train, y_train, batch_size), \n\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     callbacks=callbacks,epochs= num_epochs)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMgCh2i1K1Js"
      },
      "source": [
        "# total time 2hr 28 min but still not accurate\n",
        "\n",
        "# def plot_learning_curves(history):\n",
        "    plt.plot(history.history[\"accuracy\"],label=\"Train Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],label=\"Val Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KELIgNkERB2i"
      },
      "source": [
        "plot_learning_curves(history)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}